{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of GazeNet\n",
    "\n",
    "GazeNet involves a two-step process: first, obtaining head and body predictions, and then predicting gaze directions. This separation allows for independent experimentation with gaze predictions.\n",
    "\n",
    "This notebook shows how to do inference on GazeNet if the head and body rotations are already predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import Video\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"GazeNet\")\n",
    "from models.gazenet import GazeNet\n",
    "\n",
    "sys.path.append(\"sixDDirect_H_B\")\n",
    "from GAFA.valid_frames import get_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GazeNet from Pre-Trained Weights\n",
    "\n",
    "You can download the pre-trained weights [here](https://huggingface.co/noanonk/6DDirect_H_B/resolve/main/gazenet.ckpt). Place them in the `GazeNet/output/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GazeNet(\n",
       "  (gazemodule): GazeModule(\n",
       "    (lstm): LSTM(14, 128, num_layers=2, bidirectional=True)\n",
       "    (direction_layer): Sequential(\n",
       "      (0): Linear(in_features=1792, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=21, bias=True)\n",
       "    )\n",
       "    (kappa_layer): Sequential(\n",
       "      (0): Linear(in_features=1792, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=7, bias=True)\n",
       "      (3): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "device = torch.device(\"gpu\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = GazeNet(n_frames=7)\n",
    "model.load_state_dict(torch.load(\n",
    "    \"GazeNet/output/gazenet.ckpt\", map_location=device)[\"state_dict\"]\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Video\n",
    "Videos are saved in a pickle file and per frame. We load this from the chosen scene and video and convert the dictionary to a numpy array.\n",
    "\n",
    "If you haven't yet run the preprocessing, we have the `kitchen` folder for download [here](https://huggingface.co/noanonk/6DDirect_H_B/resolve/main/kitchen.zip). Unzip this file in the `GazeNet/data/preprocessed/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 960, 720, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"GazeNet/data/preprocessed/\"\n",
    "scene = \"kitchen/1022_2\"\n",
    "video_dir = \"Camera_3_5\"\n",
    "\n",
    "with open(os.path.join(root_dir, scene, video_dir, \"images.pkl\"), \"rb\") as f:\n",
    "    video = pickle.load(f)\n",
    "video = np.array([v for v in video.values()])\n",
    "\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Annotations\n",
    "In order to visualize the gaze predictions, we get the head bounding box from the GAFA dataset, which we've cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 4)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(root_dir, scene, video_dir, \"clean_annotations.pkl\"), \"rb\") as f:\n",
    "    annot = pickle.load(f)\n",
    "annot = annot[\"head_bb\"]\n",
    "\n",
    "annot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Video\n",
    "To visualize the video, we plot the frames one per one and write to `test.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = animation.FFMpegWriter(fps=30, codec='libx264')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axis('off')\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "with writer.saving(fig, \"test.mp4\", 100):\n",
    "    for i in range(len(video)):\n",
    "        animate(i)\n",
    "        writer.grab_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"test.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Head and Body Rotation Predictions\n",
    "The head and body rotations were pre-calculated in order to facilitate easy testing.\n",
    "We can load all the predictions for the specific video we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_B_preds = os.path.join(root_dir, scene, video_dir, \"6DDirect_H_B_preds.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we get multiple predictions from our head and body model, we only select the most certain head and body predictions per frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head_dirs : [[0.7815450429916382, -0.10810178518295288, -0.773358017206192, -0.04168832302093506, 0.8880559206008911, -0.26126569509506226], [0.8055877685546875, -0.11023050546646118, -0.7511477023363113, -0.042988717555999756, 0.8862242698669434, -0.2765321731567383], [0.8169564008712769, -0.11016595363616943, -0.7324377000331879, -0.04296988248825073, 0.8828508853912354, -0.28466349840164185], [0.8230882883071899, -0.11145138740539551, -0.7197496294975281, -0.04553323984146118, 0.8797483444213867, -0.29135894775390625], [0.8220312595367432, -0.11462944746017456, -0.7145802080631256, -0.051094889640808105, 0.8765996694564819, -0.29847949743270874], [0.8312361240386963, -0.1076962947845459, -0.7064539790153503, -0.03572636842727661, 0.8703086376190186, -0.2807849049568176], [0.852353572845459, -0.10051202774047852, -0.6792432963848114, -0.02672553062438965, 0.8701001405715942, -0.27628225088119507]]\n",
      "head_scores : [0.83739, 0.83575, 0.83231, 0.82739, 0.81911, 0.8186, 0.82673]\n",
      "body_dirs : [[0.8679598569869995, -0.12436717748641968, -0.4549005627632141, -0.10660254955291748, 0.7593623399734497, -0.4278661608695984], [0.8854836225509644, -0.10262513160705566, -0.38265562057495117, -0.08612304925918579, 0.7446433305740356, -0.41873061656951904], [0.8875125646591187, -0.09783285856246948, -0.3659648895263672, -0.08152234554290771, 0.7405780553817749, -0.4158838391304016], [0.886439323425293, -0.09719979763031006, -0.36725735664367676, -0.08015966415405273, 0.7444370985031128, -0.41517001390457153], [0.8953937292098999, -0.08828359842300415, -0.3358956575393677, -0.07064348459243774, 0.7368261814117432, -0.4105185866355896], [0.8993141651153564, -0.0799715518951416, -0.3055831789970398, -0.06273216009140015, 0.7296310663223267, -0.40636271238327026], [0.9008661508560181, -0.07879608869552612, -0.3010462522506714, -0.06196153163909912, 0.729891300201416, -0.4081534147262573]]\n",
      "body_scores : [0.91139, 0.91609, 0.9191, 0.91947, 0.92098, 0.92259, 0.92481]\n",
      "indices : [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "gaze_input = get_frames(H_B_preds, video_dir, n_frames=7)\n",
    "for k in gaze_input[0]:\n",
    "    print(k, \":\", gaze_input[0][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 753 valid inputs for our GazeNet and 800 frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753, 800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gaze_input), len(video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaze Direction Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the information GazeNet needs to predict gaze direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_frames x 6\n",
    "GazeNet_input = {\n",
    "    \"head_dirs\": torch.tensor([gaze_input[idx][\"head_dirs\"] for idx in range(len(gaze_input))]).to(device),\n",
    "    \"body_dirs\": torch.tensor([gaze_input[idx][\"body_dirs\"] for idx in range(len(gaze_input))]).to(device),\n",
    "    \"head_scores\": torch.tensor([gaze_input[idx][\"head_scores\"] for idx in range(len(gaze_input))]).unsqueeze(dim=-1).to(device),\n",
    "    \"body_scores\": torch.tensor([gaze_input[idx][\"body_scores\"] for idx in range(len(gaze_input))]).unsqueeze(dim=-1).to(device),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single forward pass of all the inputs.\n",
    "Since we are working with 7-frame data, we eventually want to know the $t$-th frame's gaze predictions in the $\\{t-3, t-2, t-1, t, t+1, t+2, t+3\\}$ sequence.\n",
    "Therefore we look at the center frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        y_hat = model(GazeNet_input)[\"direction\"][:, 7 // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_2d = y_hat[:, :2].numpy() / np.linalg.norm(y_hat, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gaze_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Gaze Predictions in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect for each valid index the center of the head and the 2D gaze vector for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_frames = {}\n",
    "for in_gaze, gaze_pred in zip(gaze_input, gaze_2d):\n",
    "    center_frame = in_gaze[\"indices\"][7 // 2]\n",
    "    \n",
    "    head_center_x = int(annot[center_frame][0] + (annot[center_frame][2] / 2))\n",
    "    head_center_y = int(annot[center_frame][1] + (annot[center_frame][3] / 2))\n",
    "\n",
    "    center_frames[center_frame] = [(head_center_x, head_center_y), gaze_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = animation.FFMpegWriter(fps=30, codec='libx264')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axis('off')\n",
    "im = plt.imshow(video[0,:,:,:])\n",
    "\n",
    "plt.close() # this is required to not display the generated image\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0,:,:,:])\n",
    "\n",
    "def arrow(i):\n",
    "    head_center = center_frames[i][0]\n",
    "    gaze_pred = center_frames[i][1]\n",
    "\n",
    "    des = (head_center[0] + int(gaze_pred[0]*50), int(head_center[1] + gaze_pred[1]*50))\n",
    "\n",
    "    video[i,:,:,:] = cv2.arrowedLine(video[i,:,:,:], head_center, des, (0, 255, 0), 3, tipLength=0.3)\n",
    "    return video[i,:,:,:]\n",
    "\n",
    "def animate(i):\n",
    "    if i in center_frames:\n",
    "        im.set_data(arrow(i))\n",
    "        return im\n",
    "    im.set_data(video[i,:,:,:])\n",
    "    return im\n",
    "\n",
    "with writer.saving(fig, \"test_w_annotations.mp4\", 100):\n",
    "    for i in range(len(video)):\n",
    "        animate(i)\n",
    "        writer.grab_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"test_w_annotations.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Video(\"test_w_annotations.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
